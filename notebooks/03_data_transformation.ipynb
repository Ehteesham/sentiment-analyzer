{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Swap 4 to 1 in target column\n",
    "2. convert all text into lower case\n",
    "3. remove username\n",
    "4. Removing Numbers\n",
    "5. removing punctuation\n",
    "7. removing urls\n",
    "6. removing extra space\n",
    "8. tokenize the text into words\n",
    "9. remove the stop words\n",
    "10. stemming the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sentimentAnalyzer.utils.common import read_dataset\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_case_converter(text):\n",
    "    return text.lower()\n",
    "\n",
    "def basic_remove_process(text):\n",
    "    # Remove users\n",
    "    no_user_text = re.sub(r'@\\w+', '', text)\n",
    "    # Remove Numbers\n",
    "    remove_number_text = re.sub(r'\\d+', '', no_user_text)\n",
    "    # Remove Punctuation\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    remove_punc_text = remove_number_text.translate(translator)\n",
    "    # Remove URLs\n",
    "    remove_url_text = re.sub(r'http\\S+|www\\S+', '', remove_punc_text)\n",
    "    # Remove Extra Space\n",
    "    final_text = \" \".join(remove_url_text.split())\n",
    "\n",
    "    return final_text\n",
    "\n",
    "def text_tokenizer(text):\n",
    "    tokenize_lst_text = word_tokenize(text)\n",
    "    return tokenize_lst_text\n",
    "\n",
    "def stop_word_removal(text_lst):\n",
    "    STOPWORD =  set(stopwords.words(\"english\"))\n",
    "    stopword_remove_text = [word for word in text_lst if word not in STOPWORD]\n",
    "    return stopword_remove_text\n",
    "\n",
    "def text_stemmer(text_lst):\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    stem_text = [stemmer.stem(word) for word in text_lst]\n",
    "    return stem_text\n",
    "\n",
    "def train_text_transformer():\n",
    "    train_df = read_dataset(Path(config.data_transformation.train_data_file), encoding=parmas.dataset.data_encoding)\n",
    "    train_df = train_df[[\"target\", \"text\"]]\n",
    "\n",
    "    train_df['target'] = train_df['target'].replace(4, 1)\n",
    "\n",
    "    train_df['text'] = train_df['text'].apply(lambda x: lower_case_converter(x))\n",
    "    train_df['text'] = train_df['text'].apply(lambda x: basic_remove_process(x))\n",
    "    train_df['text'] = train_df['text'].apply(lambda x: text_tokenizer(x))\n",
    "    train_df['text'] = train_df['text'].apply(lambda x: stop_word_removal(x))\n",
    "    train_df['text'] = train_df['text'].apply(lambda x: text_stemmer(x))\n",
    "\n",
    "    train_df['text'] = train_df['text'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "\n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_text_transformer():\n",
    "    test_df = read_dataset(Path(config.data_transformation.test_data_file), encoding=parmas.dataset.data_encoding)\n",
    "    test_df = test_df[[\"target\", \"text\"]]\n",
    "\n",
    "    test_df['target'] = test_df['target'].replace(4, 1)\n",
    "\n",
    "    test_df['text'] = test_df['text'].apply(lambda x: lower_case_converter(x))\n",
    "    test_df['text'] = test_df['text'].apply(lambda x: basic_remove_process(x))\n",
    "    test_df['text'] = test_df['text'].apply(lambda x: text_tokenizer(x))\n",
    "    test_df['text'] = test_df['text'].apply(lambda x: stop_word_removal(x))\n",
    "    test_df['text'] = test_df['text'].apply(lambda x: text_stemmer(x))\n",
    "\n",
    "    test_df['text'] = test_df['text'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)   \n",
    "\n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-03 15:25:29,497: INFO: common: Reading dataset from path: artifacts\\data_ingestion\\test\\test.csv]\n",
      "[2025-01-03 15:25:31,446: INFO: common: Dataset loaded successfully with 320000 rows and 6 columns.]\n"
     ]
    }
   ],
   "source": [
    "df_test = test_text_transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ahhh hope ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>cool tweet app razr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>know famili drama lamehey next time u hang kim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>school email wont open geographi stuff revis s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>upper airway problem</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                               text\n",
       "0       0                                       ahhh hope ok\n",
       "1       0                                cool tweet app razr\n",
       "2       0  know famili drama lamehey next time u hang kim...\n",
       "3       0  school email wont open geographi stuff revis s...\n",
       "4       0                               upper airway problem"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['target'] = df_test['target'].replace(4, 1)\n",
    "df_test['target'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df_test['text'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "y_test = df_test['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-03 02:37:05,450: INFO: common: Reading dataset from path: artifacts\\data_ingestion\\train\\train.csv]\n",
      "[2025-01-03 02:37:09,765: INFO: common: Dataset loaded successfully with 1280000 rows and 6 columns.]\n"
     ]
    }
   ],
   "source": [
    "df = train_text_transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target'] = df['target'].replace(4, 1)\n",
    "df['target'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df['text'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "y_train = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Feature_words:  300000\n"
     ]
    }
   ],
   "source": [
    "vectoriser = TfidfVectorizer(ngram_range=(1,2), max_features=300000)\n",
    "vectoriser.fit(X_train)\n",
    "print(\"No. of Feature_words: \", len(vectoriser.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'csr_matrix' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[231], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m vectoriser \u001b[38;5;241m=\u001b[39m TfidfVectorizer(ngram_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m), max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300000\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mvectoriser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\CDAC\\Machine Learning\\sentiment-analyzer\\env\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\CDAC\\Machine Learning\\sentiment-analyzer\\env\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2074\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2067\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_for_unused_params()\n\u001b[0;32m   2068\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2069\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2070\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2071\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2072\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2073\u001b[0m )\n\u001b[1;32m-> 2074\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2075\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2076\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32md:\\CDAC\\Machine Learning\\sentiment-analyzer\\env\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\CDAC\\Machine Learning\\sentiment-analyzer\\env\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1376\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1368\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1369\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1370\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1371\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1372\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1373\u001b[0m             )\n\u001b[0;32m   1374\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1376\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1379\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32md:\\CDAC\\Machine Learning\\sentiment-analyzer\\env\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1263\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1261\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1262\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1263\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1264\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1265\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32md:\\CDAC\\Machine Learning\\sentiment-analyzer\\env\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:104\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m preprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 104\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    106\u001b[0m         doc \u001b[38;5;241m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[1;32md:\\CDAC\\Machine Learning\\sentiment-analyzer\\env\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:62\u001b[0m, in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;124;03mapply to a document.\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;124;03m    preprocessed string\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lower:\n\u001b[1;32m---> 62\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m()\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accent_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     64\u001b[0m     doc \u001b[38;5;241m=\u001b[39m accent_function(doc)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'csr_matrix' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "vectoriser = TfidfVectorizer(ngram_range=(1,2), max_features=300000)\n",
    "vectoriser.fit(X_train)\n",
    "X_train = vectoriser.transform(X_train)\n",
    "X_test = vectoriser.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectoriser.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 12996765 stored elements and shape (1280000, 300000)>\n",
      "  Coords\tValues\n",
      "  (0, 21927)\t0.18834560650948634\n",
      "  (0, 38994)\t0.2266529703014415\n",
      "  (0, 97600)\t0.09953069629947216\n",
      "  (0, 97667)\t0.3033362119954497\n",
      "  (0, 140496)\t0.10172579504667327\n",
      "  (0, 141829)\t0.33359089799648034\n",
      "  (0, 187613)\t0.22462141622920775\n",
      "  (0, 187618)\t0.244774576110176\n",
      "  (0, 198100)\t0.22288007501087348\n",
      "  (0, 198226)\t0.31102411999397145\n",
      "  (0, 202841)\t0.31784687670853956\n",
      "  (0, 205118)\t0.14660561575350095\n",
      "  (0, 233354)\t0.14906717790505142\n",
      "  (0, 233427)\t0.2061554340305347\n",
      "  (0, 261074)\t0.3375114115145163\n",
      "  (0, 293832)\t0.15596546687088778\n",
      "  (0, 296255)\t0.14099012189396565\n",
      "  (0, 296652)\t0.29574464121558597\n",
      "  (1, 2340)\t0.25944224336969496\n",
      "  (1, 2437)\t0.49262739704644987\n",
      "  (1, 65739)\t0.3490510347355771\n",
      "  (1, 69504)\t0.47757585145198855\n",
      "  (1, 78091)\t0.2809553359383544\n",
      "  (1, 78102)\t0.4562173683488655\n",
      "  (1, 218813)\t0.23026607070750216\n",
      "  :\t:\n",
      "  (1279998, 28843)\t0.2137390928378798\n",
      "  (1279998, 56998)\t0.2993865987989061\n",
      "  (1279998, 76405)\t0.2402169968229301\n",
      "  (1279998, 120009)\t0.26083544290643934\n",
      "  (1279998, 162040)\t0.13301147868675656\n",
      "  (1279998, 162383)\t0.4036818818159024\n",
      "  (1279998, 192776)\t0.34042685801097455\n",
      "  (1279998, 245011)\t0.25644540688358686\n",
      "  (1279998, 245025)\t0.40922796435007963\n",
      "  (1279998, 245973)\t0.1739512519977191\n",
      "  (1279998, 269561)\t0.3177973947086537\n",
      "  (1279998, 274908)\t0.27913504673646206\n",
      "  (1279999, 74794)\t0.2223877147444358\n",
      "  (1279999, 74915)\t0.4511375159081803\n",
      "  (1279999, 94539)\t0.11797945848223185\n",
      "  (1279999, 95256)\t0.33753016705058014\n",
      "  (1279999, 114956)\t0.158512315598126\n",
      "  (1279999, 115641)\t0.3672530901748761\n",
      "  (1279999, 122251)\t0.10986507978124288\n",
      "  (1279999, 123061)\t0.2014747699309182\n",
      "  (1279999, 156802)\t0.24290852085704606\n",
      "  (1279999, 222167)\t0.2150365869449316\n",
      "  (1279999, 239536)\t0.21367844937172095\n",
      "  (1279999, 239660)\t0.3468766840757447\n",
      "  (1279999, 288767)\t0.3678073921952759\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = vectoriser.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 3183520 stored elements and shape (320000, 300000)>\n",
      "  Coords\tValues\n",
      "  (0, 3487)\t0.38802061594204845\n",
      "  (0, 3517)\t0.6670862598470247\n",
      "  (0, 116329)\t0.23914869327494115\n",
      "  (0, 116802)\t0.5106112743989338\n",
      "  (0, 181559)\t0.2941427416898907\n",
      "  (1, 11142)\t0.299545373022814\n",
      "  (1, 46898)\t0.23186242926404135\n",
      "  (1, 47240)\t0.5083842036366177\n",
      "  (1, 204667)\t0.505148857805784\n",
      "  (1, 264806)\t0.21724585971007923\n",
      "  (1, 264833)\t0.5437708642300568\n",
      "  (2, 34053)\t0.15125823363111143\n",
      "  (2, 62890)\t0.2208342430190118\n",
      "  (2, 75320)\t0.16822707725044758\n",
      "  (2, 75374)\t0.341832834380506\n",
      "  (2, 103245)\t0.14462932659491592\n",
      "  (2, 103517)\t0.2631356377436169\n",
      "  (2, 106090)\t0.17580805983010853\n",
      "  (2, 121609)\t0.13139796657418784\n",
      "  (2, 121681)\t0.26165086841161433\n",
      "  (2, 133168)\t0.24217208660440823\n",
      "  (2, 134231)\t0.11748648132058323\n",
      "  (2, 134519)\t0.324934322030119\n",
      "  (2, 140496)\t0.10627536619059096\n",
      "  (2, 175407)\t0.14555629701945746\n",
      "  :\t:\n",
      "  (319996, 281461)\t0.2331920895350605\n",
      "  (319996, 281767)\t0.5550666936415664\n",
      "  (319997, 142995)\t0.4265457755395528\n",
      "  (319997, 203723)\t0.6186858722751944\n",
      "  (319997, 249032)\t0.2737549830330028\n",
      "  (319997, 249456)\t0.6002871830058037\n",
      "  (319998, 22774)\t0.18564927557386357\n",
      "  (319998, 23084)\t0.3390211124541282\n",
      "  (319998, 36418)\t0.22044172683918403\n",
      "  (319998, 36435)\t0.4156639169538776\n",
      "  (319998, 61428)\t0.13902638770220932\n",
      "  (319998, 61521)\t0.2900442799414101\n",
      "  (319998, 71015)\t0.1939025540438372\n",
      "  (319998, 102149)\t0.28368209921136417\n",
      "  (319998, 205733)\t0.1497629020784558\n",
      "  (319998, 205876)\t0.26719853896433976\n",
      "  (319998, 223505)\t0.17864857036994766\n",
      "  (319998, 223649)\t0.3424429565990874\n",
      "  (319998, 281461)\t0.18025635011782934\n",
      "  (319998, 281787)\t0.3635951076087894\n",
      "  (319999, 31042)\t0.4023279897676758\n",
      "  (319999, 31086)\t0.6547926897798095\n",
      "  (319999, 93553)\t0.330152705193104\n",
      "  (319999, 142995)\t0.37373302252941243\n",
      "  (319999, 269578)\t0.40087596733294967\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Path(\"artifacts/data_transformation/train/X_train.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.npz'"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'DataInfo' from 'sentimentAnalyzer.utils.common' (D:\\CDAC\\Machine Learning\\sentiment-analyzer\\src\\sentimentAnalyzer\\utils\\common.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[208], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentimentAnalyzer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logger\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentimentAnalyzer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataInfo, read_dataset\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'DataInfo' from 'sentimentAnalyzer.utils.common' (D:\\CDAC\\Machine Learning\\sentiment-analyzer\\src\\sentimentAnalyzer\\utils\\common.py)"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import save_npz, load_npz\n",
    "import numpy as np\n",
    "from sentimentAnalyzer.logging import logger\n",
    "from sentimentAnalyzer.utils.common import DataInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataInfo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[210], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[43mDataInfo\u001b[49m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DataInfo' is not defined"
     ]
    }
   ],
   "source": [
    "a = DataInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_transformed_data_file(path: Path, data, data_info: DataInfo) -> None:\n",
    "    if path.suffix == \".npz\":\n",
    "        save_npz(path, data)\n",
    "        logger.info(f\"Input Data is Saved in {path}\")\n",
    "    elif path.suffix == \".npy\":\n",
    "        np.save(path, data)\n",
    "        logger.info(f\"Output Data is Saved in {path}\")\n",
    "    else:\n",
    "        logger.info(f\"{path.suffix} is not a correct extension {data_info} will not be stored\")\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import save_npz, load_npz\n",
    "import numpy as np\n",
    "\n",
    "save_npz(\"X_train.npz\", X_train)\n",
    "np.save(\"y_train.npy\", y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_npz(\"X_test.npz\", X_train)\n",
    "np.save(\"y_test.npy\", y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_loaded = load_npz(\"X_train.npz\")\n",
    "y_train_loaded = np.load(\"y_train.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_loaded = load_npz(\"X_test.npz\")\n",
    "y_test_loaded = np.load(\"y_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0], shape=(1280000,))"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1280000, 300000)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_loaded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluation(model):\n",
    "\n",
    "    y_pred = model.predict(X_test_loaded)\n",
    "\n",
    "    print(classification_report(y_test_loaded, y_pred))\n",
    "\n",
    "    cf_matrix = confusion_matrix(y_test_loaded, y_pred)\n",
    "\n",
    "    print(cf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.82      0.83    640506\n",
      "           1       0.82      0.85      0.84    639494\n",
      "\n",
      "    accuracy                           0.83   1280000\n",
      "   macro avg       0.83      0.83      0.83   1280000\n",
      "weighted avg       0.83      0.83      0.83   1280000\n",
      "\n",
      "[[524120 116386]\n",
      " [ 96995 542499]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lrmodel = LogisticRegression(C = 2, max_iter=1000, n_jobs=-1)\n",
    "lrmodel.fit(X_train_loaded, y_train_loaded)\n",
    "model_evaluation(lrmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ehtes\\AppData\\Local\\Temp\\ipykernel_16636\\4249706494.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['target'] = data['target'].replace(4, 1)\n"
     ]
    }
   ],
   "source": [
    "data = df[['target', 'text']]\n",
    "data['target'] = data['target'].replace(4, 1)\n",
    "# print(data['target'].unique())\n",
    "\n",
    "# Seperating the Positive and Negative Tweet Data\n",
    "data_pos = data[data['target'] == 1]\n",
    "data_neg = data[data['target'] == 0]\n",
    "# print(data_neg,data_pos)\n",
    "\n",
    "# Taking Less values for calculation purpose\n",
    "data_pos = data_pos.iloc[:int(20000)]\n",
    "data_neg = data_neg.iloc[:int(20000)]\n",
    "\n",
    "# Making text in lower case for consistency\n",
    "\n",
    "dataset = pd.concat([data_pos, data_neg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>@jbtaylor WIth ya. &amp;quot;I'd like a Palm Pre, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>felt the earthquake this afternoon, it seems t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Ruffles on shirts are like so in, me Likey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Pretty bad night into a crappy morning....FML!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>@dcbriccetti yeah, what a clear view!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                               text\n",
       "0       1  @jbtaylor WIth ya. &quot;I'd like a Palm Pre, ...\n",
       "1       1  felt the earthquake this afternoon, it seems t...\n",
       "2       1        Ruffles on shirts are like so in, me Likey \n",
       "3       0  Pretty bad night into a crappy morning....FML!...\n",
       "4       1             @dcbriccetti yeah, what a clear view! "
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ehtes\\AppData\\Local\\Temp\\ipykernel_16636\\3612197400.py:1: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  dataset[\"target\"][0] = 0\n",
      "C:\\Users\\ehtes\\AppData\\Local\\Temp\\ipykernel_16636\\3612197400.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"target\"][0] = 0\n"
     ]
    }
   ],
   "source": [
    "dataset[\"target\"][0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@jbtaylor WIth ya. &amp;quot;I'd like a Palm Pre, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>felt the earthquake this afternoon, it seems t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Ruffles on shirts are like so in, me Likey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>@dcbriccetti yeah, what a clear view!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>One more time Follow my fam #FF @georgeann13 @...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                               text\n",
       "0       0  @jbtaylor WIth ya. &quot;I'd like a Palm Pre, ...\n",
       "1       1  felt the earthquake this afternoon, it seems t...\n",
       "2       1        Ruffles on shirts are like so in, me Likey \n",
       "4       1             @dcbriccetti yeah, what a clear view! \n",
       "5       1  One more time Follow my fam #FF @georgeann13 @..."
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@jbtaylor WIth ya. &quot;I'd like a Palm Pre, Touchstone charger. ReadyNow? Yes, that sounds good. But is my beer ready now?'  #prelaunch123\""
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = df['text'][0]+\"123\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@jbtaylor with ya. &quot;i'd like a palm pre, touchstone charger. readynow? yes, that sounds good. but is my beer ready now?'  #prelaunch123\""
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lower_text = text.lower()\n",
    "lower_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" with ya. &quot;i'd like a palm pre, touchstone charger. readynow? yes, that sounds good. but is my beer ready now?'  #prelaunch123\""
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_user_text = re.sub(r'@\\w+', '', lower_text)\n",
    "no_user_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" with ya. &quot;i'd like a palm pre, touchstone charger. readynow? yes, that sounds good. but is my beer ready now?'  #prelaunch\""
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_number_text = re.sub(r'\\d+', '', no_user_text)\n",
    "remove_number_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' with ya quotid like a palm pre touchstone charger readynow yes that sounds good but is my beer ready now  prelaunch'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator = str.maketrans('', '', string.punctuation)\n",
    "remove_punc_text = remove_number_text.translate(translator)\n",
    "remove_punc_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'with ya quotid like a palm pre touchstone charger readynow yes that sounds good but is my beer ready now prelaunch'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_space_text = \" \".join(remove_punc_text.split())\n",
    "remove_space_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'with ya quotid like a palm pre touchstone charger readynow yes that sounds good but is my beer ready now prelaunch'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_url_text = re.sub(r'http\\S+|www\\S+', '', remove_space_text)\n",
    "remove_url_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['with',\n",
       " 'ya',\n",
       " 'quotid',\n",
       " 'like',\n",
       " 'a',\n",
       " 'palm',\n",
       " 'pre',\n",
       " 'touchstone',\n",
       " 'charger',\n",
       " 'readynow',\n",
       " 'yes',\n",
       " 'that',\n",
       " 'sounds',\n",
       " 'good',\n",
       " 'but',\n",
       " 'is',\n",
       " 'my',\n",
       " 'beer',\n",
       " 'ready',\n",
       " 'now',\n",
       " 'prelaunch']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_text = word_tokenize(remove_url_text)\n",
    "tokenize_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ya',\n",
       " 'quotid',\n",
       " 'like',\n",
       " 'palm',\n",
       " 'pre',\n",
       " 'touchstone',\n",
       " 'charger',\n",
       " 'readynow',\n",
       " 'yes',\n",
       " 'sounds',\n",
       " 'good',\n",
       " 'beer',\n",
       " 'ready',\n",
       " 'prelaunch']"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STOPWORD =  set(stopwords.words(\"english\"))\n",
    "stopword_remove_text = [word for word in tokenize_text if word not in STOPWORD]\n",
    "stopword_remove_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ya',\n",
       " 'quotid',\n",
       " 'like',\n",
       " 'palm',\n",
       " 'pre',\n",
       " 'touchston',\n",
       " 'charger',\n",
       " 'readynow',\n",
       " 'yes',\n",
       " 'sound',\n",
       " 'good',\n",
       " 'beer',\n",
       " 'readi',\n",
       " 'prelaunch']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "stem_text = [stemmer.stem(word) for word in stopword_remove_text]\n",
    "stem_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectoriser = TfidfVectorizer(ngram_range=(1,2), max_features=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\CDAC\\\\Machine Learning\\\\sentiment-analyzer\\\\notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\CDAC\\\\Machine Learning\\\\sentiment-analyzer'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    dataset_dir: Path\n",
    "    transformed_data_dir: Path\n",
    "    train_data_file: Path\n",
    "    test_data_file: Path\n",
    "    train_transformed_dir: Path\n",
    "    test_transformed_dir: Path\n",
    "    encoder: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentimentAnalyzer.constant import *\n",
    "from sentimentAnalyzer.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self, config_path = CONFIG_FILE_PATH, params_path = PARAMS_FILE_PATH):\n",
    "        self.config = read_yaml(config_path)\n",
    "        self.params = read_yaml(params_path)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "        dataset = self.params.dataset\n",
    "        create_directories([config.transformed_data_dir])\n",
    "        create_directories([config.train_transformed_dir])\n",
    "        create_directories([config.test_transformed_dir])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            dataset_dir = Path(config.dataset_dir),\n",
    "            transformed_data_dir = Path(config.transformed_data_dir),\n",
    "            train_data_file = Path(config.train_data_file),\n",
    "            test_data_file = Path(config.test_data_file),\n",
    "            train_transformed_dir = Path(config.train_transformed_dir),\n",
    "            test_transformed_dir = Path(config.test_transformed_dir),\n",
    "            encoder = dataset.data_encoding\n",
    "        )\n",
    "\n",
    "        return data_transformation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "from sentimentAnalyzer.logging import logger\n",
    "from sentimentAnalyzer.utils.common import read_dataset, save_transformed_data_file, DataInfo\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def lower_case_converter(self, text):\n",
    "        return text.lower()\n",
    "\n",
    "    def basic_remove_process(self, text):\n",
    "        # Remove users\n",
    "        no_user_text = re.sub(r'@\\w+', '', text)\n",
    "        # Remove Numbers\n",
    "        remove_number_text = re.sub(r'\\d+', '', no_user_text)\n",
    "        # Remove Punctuation\n",
    "        translator = str.maketrans('', '', string.punctuation)\n",
    "        remove_punc_text = remove_number_text.translate(translator)\n",
    "        # Remove URLs\n",
    "        remove_url_text = re.sub(r'http\\S+|www\\S+', '', remove_punc_text)\n",
    "        # Remove Extra Space\n",
    "        final_text = \" \".join(remove_url_text.split())\n",
    "\n",
    "        return final_text\n",
    "\n",
    "    def text_tokenizer(self, text):\n",
    "        tokenize_lst_text = word_tokenize(text)\n",
    "        return tokenize_lst_text\n",
    "\n",
    "    def stop_word_removal(self, text_lst):\n",
    "        STOPWORD =  set(stopwords.words(\"english\"))\n",
    "        stopword_remove_text = [word for word in text_lst if word not in STOPWORD]\n",
    "        return stopword_remove_text\n",
    "\n",
    "    def text_stemmer(self, text_lst):\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stem_text = [stemmer.stem(word) for word in text_lst]\n",
    "        return stem_text\n",
    "    \n",
    "    def text_transformer(self, path: Path, data_info: DataInfo):\n",
    "        df = read_dataset(path, encoding=self.config.encoder)\n",
    "        df = df[[\"target\", \"text\"]]\n",
    "\n",
    "        df['target'] = df['target'].replace(4, 1)\n",
    "\n",
    "        df['text'] = df['text'].apply(lambda x: self.lower_case_converter(x))\n",
    "        df['text'] = df['text'].apply(lambda x: self.basic_remove_process(x))\n",
    "        df['text'] = df['text'].apply(lambda x: self.text_tokenizer(x))\n",
    "        df['text'] = df['text'].apply(lambda x: self.stop_word_removal(x))\n",
    "        df['text'] = df['text'].apply(lambda x: self.text_stemmer(x))\n",
    "\n",
    "        df['text'] = df['text'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "        logger.info(f\"{data_info.value} has been Pre-Processed\")\n",
    "\n",
    "        return df['text'], df['target']\n",
    "    \n",
    "\n",
    "    def data_transformation(self) -> None:\n",
    "        X_train, y_train = self.text_transformer(path=self.config.train_data_file, data_info=DataInfo.TRAINING)\n",
    "        X_test, y_test = self.text_transformer(path=self.config.test_data_file, data_info=DataInfo.TESTING)\n",
    "        \n",
    "        vectoriser = TfidfVectorizer(ngram_range=(1,2), max_features=300000)\n",
    "        vectoriser.fit(X_train)\n",
    "        X_train = vectoriser.transform(X_train)\n",
    "        X_test = vectoriser.transform(X_test)\n",
    "\n",
    "        # Saving the Data\n",
    "        # Training Data\n",
    "        x_tr_path = Path(f\"{self.config.train_transformed_dir}/X_train.npz\")\n",
    "        y_tr_path = Path(f\"{self.config.train_transformed_dir}/y_train.npy\")\n",
    "        save_transformed_data_file(x_tr_path, X_train, data_info=DataInfo.TRAINING)\n",
    "        save_transformed_data_file(y_tr_path, y_train, data_info=DataInfo.TRAINING)\n",
    "        # Testing Day\n",
    "        x_test_path = Path(f\"{self.config.test_transformed_dir}/X_test.npz\")\n",
    "        y_test_path = Path(f\"{self.config.test_transformed_dir}/y_test.npy\")\n",
    "        save_transformed_data_file(x_test_path, X_test, data_info=DataInfo.TESTING)\n",
    "        save_transformed_data_file(y_test_path, y_test, data_info=DataInfo.TESTING)\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-03 20:20:34,451: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-01-03 20:20:34,453: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-01-03 20:20:34,454: INFO: common: created directory at: artifacts]\n",
      "[2025-01-03 20:20:34,456: INFO: common: created directory at: artifacts/data_transformation]\n",
      "[2025-01-03 20:20:34,457: INFO: common: created directory at: artifacts/data_transformation/train]\n",
      "[2025-01-03 20:20:34,458: INFO: common: created directory at: artifacts/data_transformation/test]\n",
      "[2025-01-03 20:20:34,459: INFO: common: Reading dataset from path: artifacts\\data_ingestion\\train\\train.csv]\n",
      "[2025-01-03 20:20:38,523: INFO: common: Dataset loaded successfully with 1280000 rows and 6 columns.]\n",
      "[2025-01-03 20:25:40,423: INFO: 3014423729: Transformed Training Data has been Pre-Processed]\n",
      "[2025-01-03 20:25:40,423: INFO: common: Reading dataset from path: artifacts\\data_ingestion\\test\\test.csv]\n",
      "[2025-01-03 20:25:41,238: INFO: common: Dataset loaded successfully with 320000 rows and 6 columns.]\n",
      "[2025-01-03 20:26:57,235: INFO: 3014423729: Transformed Testing Data has been Pre-Processed]\n",
      "[2025-01-03 20:28:06,939: INFO: common: Transformed Training Data is Saved in artifacts\\data_transformation\\train\\X_train.npz]\n",
      "[2025-01-03 20:28:06,940: INFO: common: Transformed Training Data is Saved in artifacts\\data_transformation\\train\\y_train.npy]\n",
      "[2025-01-03 20:28:09,128: INFO: common: Transformed Testing Data is Saved in artifacts\\data_transformation\\test\\X_test.npz]\n",
      "[2025-01-03 20:28:09,131: INFO: common: Transformed Testing Data is Saved in artifacts\\data_transformation\\test\\y_test.npy]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    data_validation = DataTransformation(config=data_transformation_config)\n",
    "    data_validation.data_transformation()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentimentAnalyzer.utils.common import load_transformed_data_file, DataInfo\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-03 20:45:52,910: INFO: common: Transformed Training Data is load from artifacts\\data_transformation\\train\\X_train.npz]\n",
      "[2025-01-03 20:45:52,921: INFO: common: Transformed Training Data is load from artifacts\\data_transformation\\train\\y_train.npy]\n"
     ]
    }
   ],
   "source": [
    "X_train = load_transformed_data_file(Path(f\"artifacts/data_transformation/train/X_train.npz\"), data_info=DataInfo.TRAINING)\n",
    "y_train = load_transformed_data_file(Path(f\"artifacts/data_transformation/train/y_train.npy\"), data_info=DataInfo.TRAINING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-03 20:45:53,618: INFO: common: Transformed Testing Data is load from artifacts\\data_transformation\\test\\X_test.npz]\n",
      "[2025-01-03 20:45:53,627: INFO: common: Transformed Testing Data is load from artifacts\\data_transformation\\test\\y_test.npy]\n"
     ]
    }
   ],
   "source": [
    "X_test = load_transformed_data_file(Path(f\"artifacts/data_transformation/test/X_test.npz\"), data_info=DataInfo.TESTING)\n",
    "y_test = load_transformed_data_file(Path(f\"artifacts/data_transformation/test/y_test.npy\"), data_info=DataInfo.TESTING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path(f\"artifacts/data_transformation/test/X_test.npz\").exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse._csr.csr_matrix"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluation(model):\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(cf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.78      0.79    159494\n",
      "           1       0.79      0.81      0.80    160506\n",
      "\n",
      "    accuracy                           0.80    320000\n",
      "   macro avg       0.80      0.80      0.80    320000\n",
      "weighted avg       0.80      0.80      0.80    320000\n",
      "\n",
      "[[124195  35299]\n",
      " [ 30173 130333]]\n"
     ]
    }
   ],
   "source": [
    "lrmodel = LogisticRegression(C = 2, max_iter=1000, n_jobs=-1)\n",
    "lrmodel.fit(X_train, y_train)\n",
    "model_evaluation(lrmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'artifacts_root': 'artifacts', 'data_ingestion': {'root_dir': 'artifacts/data_ingestion', 'source_dir': 'raw.zip', 'unzip_dir': 'artifacts/data_ingestion', 'dataset_file': 'artifacts/data_ingestion/raw.csv', 'train_data_dir': 'artifacts/data_ingestion/train', 'test_data_dir': 'artifacts/data_ingestion/test'}, 'data_validation': {'root_dir': 'artifacts/data_validation', 'STATUS_FILE': 'artifacts/data_validation/status.txt', 'file_check_dir': 'artifacts/data_ingestion', 'FILE_NAMES': ['test', 'train']}, 'data_transformation': {'dataset_dir': 'artifacts/data_ingestion', 'transformed_data_dir': 'artifacts/data_transformation'}},"
     ]
    }
   ],
   "source": [
    "print(config, end=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConfigBox({'dataset': {'data_encoding': 'ISO-8859-1', 'train_size': 0.8, 'test_size': 0.2}})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_trns = config.data_transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConfigBox({'dataset_dir': 'artifacts/data_ingestion', 'transformed_data_dir': 'artifacts/data_transformation'})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_trns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_file = Path(os.path.join(data_trns.dataset_dir, \"train/train.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('artifacts/data_ingestion/train/train.csv')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentimentAnalyzer.utils.common import read_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-03 01:18:55,775: INFO: common: Reading dataset from path: artifacts\\data_ingestion\\train\\train.csv]\n",
      "[2025-01-03 01:18:59,894: INFO: common: Dataset loaded successfully with 1280000 rows and 6 columns.]\n"
     ]
    }
   ],
   "source": [
    "df = read_dataset(path=train_data_file, encoding=parmas.dataset.data_encoding, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>2051457557</td>\n",
       "      <td>Fri Jun 05 22:04:23 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>JGoldsborough</td>\n",
       "      <td>@jbtaylor WIth ya. &amp;quot;I'd like a Palm Pre, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>2053083567</td>\n",
       "      <td>Sat Jun 06 03:12:21 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Psioui</td>\n",
       "      <td>felt the earthquake this afternoon, it seems t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>1976779404</td>\n",
       "      <td>Sat May 30 19:02:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>adriville</td>\n",
       "      <td>Ruffles on shirts are like so in, me Likey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2325739990</td>\n",
       "      <td>Thu Jun 25 05:59:18 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Blondie128</td>\n",
       "      <td>Pretty bad night into a crappy morning....FML!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1973503391</td>\n",
       "      <td>Sat May 30 11:16:35 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>khrabrov</td>\n",
       "      <td>@dcbriccetti yeah, what a clear view!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target         ids                          date      flag           user  \\\n",
       "0       4  2051457557  Fri Jun 05 22:04:23 PDT 2009  NO_QUERY  JGoldsborough   \n",
       "1       4  2053083567  Sat Jun 06 03:12:21 PDT 2009  NO_QUERY         Psioui   \n",
       "2       4  1976779404  Sat May 30 19:02:49 PDT 2009  NO_QUERY      adriville   \n",
       "3       0  2325739990  Thu Jun 25 05:59:18 PDT 2009  NO_QUERY     Blondie128   \n",
       "4       4  1973503391  Sat May 30 11:16:35 PDT 2009  NO_QUERY       khrabrov   \n",
       "\n",
       "                                                text  \n",
       "0  @jbtaylor WIth ya. &quot;I'd like a Palm Pre, ...  \n",
       "1  felt the earthquake this afternoon, it seems t...  \n",
       "2        Ruffles on shirts are like so in, me Likey   \n",
       "3  Pretty bad night into a crappy morning....FML!...  \n",
       "4             @dcbriccetti yeah, what a clear view!   "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1280000 entries, 0 to 1279999\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count    Dtype \n",
      "---  ------  --------------    ----- \n",
      " 0   target  1280000 non-null  int64 \n",
      " 1   ids     1280000 non-null  int64 \n",
      " 2   date    1280000 non-null  object\n",
      " 3   flag    1280000 non-null  object\n",
      " 4   user    1280000 non-null  object\n",
      " 5   text    1280000 non-null  object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 58.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replacing 4 to 1 in target \n",
    "df['target'] = df['target'].replace(4, 1)\n",
    "df['target'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2051457557</td>\n",
       "      <td>Fri Jun 05 22:04:23 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>JGoldsborough</td>\n",
       "      <td>@jbtaylor WIth ya. &amp;quot;I'd like a Palm Pre, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2053083567</td>\n",
       "      <td>Sat Jun 06 03:12:21 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Psioui</td>\n",
       "      <td>felt the earthquake this afternoon, it seems t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1976779404</td>\n",
       "      <td>Sat May 30 19:02:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>adriville</td>\n",
       "      <td>Ruffles on shirts are like so in, me Likey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2325739990</td>\n",
       "      <td>Thu Jun 25 05:59:18 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Blondie128</td>\n",
       "      <td>Pretty bad night into a crappy morning....FML!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1973503391</td>\n",
       "      <td>Sat May 30 11:16:35 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>khrabrov</td>\n",
       "      <td>@dcbriccetti yeah, what a clear view!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target         ids                          date      flag           user  \\\n",
       "0       1  2051457557  Fri Jun 05 22:04:23 PDT 2009  NO_QUERY  JGoldsborough   \n",
       "1       1  2053083567  Sat Jun 06 03:12:21 PDT 2009  NO_QUERY         Psioui   \n",
       "2       1  1976779404  Sat May 30 19:02:49 PDT 2009  NO_QUERY      adriville   \n",
       "3       0  2325739990  Thu Jun 25 05:59:18 PDT 2009  NO_QUERY     Blondie128   \n",
       "4       1  1973503391  Sat May 30 11:16:35 PDT 2009  NO_QUERY       khrabrov   \n",
       "\n",
       "                                                text  \n",
       "0  @jbtaylor WIth ya. &quot;I'd like a Palm Pre, ...  \n",
       "1  felt the earthquake this afternoon, it seems t...  \n",
       "2        Ruffles on shirts are like so in, me Likey   \n",
       "3  Pretty bad night into a crappy morning....FML!...  \n",
       "4             @dcbriccetti yeah, what a clear view!   "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2051457557</td>\n",
       "      <td>Fri Jun 05 22:04:23 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>JGoldsborough</td>\n",
       "      <td>@jbtaylor with ya. &amp;quot;i'd like a palm pre, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2053083567</td>\n",
       "      <td>Sat Jun 06 03:12:21 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Psioui</td>\n",
       "      <td>felt the earthquake this afternoon, it seems t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1976779404</td>\n",
       "      <td>Sat May 30 19:02:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>adriville</td>\n",
       "      <td>ruffles on shirts are like so in, me likey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2325739990</td>\n",
       "      <td>Thu Jun 25 05:59:18 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Blondie128</td>\n",
       "      <td>pretty bad night into a crappy morning....fml!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1973503391</td>\n",
       "      <td>Sat May 30 11:16:35 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>khrabrov</td>\n",
       "      <td>@dcbriccetti yeah, what a clear view!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target         ids                          date      flag           user  \\\n",
       "0       1  2051457557  Fri Jun 05 22:04:23 PDT 2009  NO_QUERY  JGoldsborough   \n",
       "1       1  2053083567  Sat Jun 06 03:12:21 PDT 2009  NO_QUERY         Psioui   \n",
       "2       1  1976779404  Sat May 30 19:02:49 PDT 2009  NO_QUERY      adriville   \n",
       "3       0  2325739990  Thu Jun 25 05:59:18 PDT 2009  NO_QUERY     Blondie128   \n",
       "4       1  1973503391  Sat May 30 11:16:35 PDT 2009  NO_QUERY       khrabrov   \n",
       "\n",
       "                                                text  \n",
       "0  @jbtaylor with ya. &quot;i'd like a palm pre, ...  \n",
       "1  felt the earthquake this afternoon, it seems t...  \n",
       "2        ruffles on shirts are like so in, me likey   \n",
       "3  pretty bad night into a crappy morning....fml!...  \n",
       "4             @dcbriccetti yeah, what a clear view!   "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# changing to lower case of text columns\n",
    "\n",
    "df['text'] = df['text'].apply(lambda x: x.lower())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to d:\\CDAC\\Machine\n",
      "[nltk_data]     Learning\\sentiment-analyzer...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords', download_dir=os.getcwd())\n",
    "stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n"
     ]
    }
   ],
   "source": [
    "print(len(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORD = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@jbtaylor with ya. &quot;i'd like a palm pre, touchstone charger. readynow? yes, that sounds good. but is my beer ready now?'  #prelaunch\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\ehtes\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "tryout = word_tokenize(df['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@',\n",
       " 'jbtaylor',\n",
       " 'with',\n",
       " 'ya',\n",
       " '.',\n",
       " '&',\n",
       " 'quot',\n",
       " ';',\n",
       " 'i',\n",
       " \"'d\",\n",
       " 'like',\n",
       " 'a',\n",
       " 'palm',\n",
       " 'pre',\n",
       " ',',\n",
       " 'touchstone',\n",
       " 'charger',\n",
       " '.',\n",
       " 'readynow',\n",
       " '?',\n",
       " 'yes',\n",
       " ',',\n",
       " 'that',\n",
       " 'sounds',\n",
       " 'good',\n",
       " '.',\n",
       " 'but',\n",
       " 'is',\n",
       " 'my',\n",
       " 'beer',\n",
       " 'ready',\n",
       " 'now',\n",
       " '?',\n",
       " \"'\",\n",
       " '#',\n",
       " 'prelaunch']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tryout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_try = \"\".join([word for word in df['text'][0].split() if word not in STOPWORD])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@jbtaylorya.&quot;i'dlikepalmpre,touchstonecharger.readynow?yes,soundsgood.beerreadynow?'#prelaunch\""
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_try = \" \".join([word for word in tryout if word not in STOPWORD])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@ jbtaylor ya . & quot ; 'd like palm pre , touchstone charger . readynow ? yes , sounds good . beer ready ? ' # prelaunch\""
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = \" \".join([word for word in tryout if word not in string.punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"jbtaylor with ya quot i 'd like a palm pre touchstone charger readynow yes that sounds good but is my beer ready now prelaunch\""
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"'d\" not in string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = str.maketrans('', '', string.punctuation)\n",
    "clean_data = stop_try.translate(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' jbtaylor ya   quot  d like palm pre  touchstone charger  readynow  yes  sounds good  beer ready    prelaunch'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = re.sub(r'(.)1+', r'1', clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  j b t a y l o r   y a       q u o t     d   l i k e   p a l m   p r e     t o u c h s t o n e   c h a r g e r     r e a d y n o w     y e s     s o u n d s   g o o d     b e e r   r e a d y         p r e l a u n c h'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insight of raw Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv(\"artifacts/data_ingestion/raw.csv\", encoding=parmas.dataset.data_encoding,\n",
    "                     names=[\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target         ids                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1600000 entries, 0 to 1599999\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count    Dtype \n",
      "---  ------  --------------    ----- \n",
      " 0   target  1600000 non-null  int64 \n",
      " 1   ids     1600000 non-null  int64 \n",
      " 2   date    1600000 non-null  object\n",
      " 3   flag    1600000 non-null  object\n",
      " 4   user    1600000 non-null  object\n",
      " 5   text    1600000 non-null  object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 73.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
